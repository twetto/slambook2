# 第二講：初識SLAM

本講大致分兩部分，第一：將SLAM模組化，並分別介紹它們；第二：準備開發環境。

首先破題，Simultaneous Localization And Mapping (SLAM) 包含了Localization(定位)及Mapping(建圖)兩部分；定位理解自身的**狀態**，建圖理解外在的**環境**。爲了理解這些，我們會需要幫機器安裝一些感測器。SLAM中特別強調這台機器要在未知環境工作，所以諸如GPS、軌道這些安裝在環境的約束我們儘量不用，而是強調使用相機。我自己額外希望能把條件約束在使用單目相機上；因爲其尺度不確定性，所以會比雙目或使用光達/紅外線深度感測還要難一點。另外，雖然本書沒有提到，但單目相機若跟IMU做融合的話其實是有解析解(closed-form solution)的(請參考[蘇黎世理工的課程簡報](https://rpg.ifi.uzh.ch/teaching.html)，大概是Lecture 13 - Visual Inertial fusion那邊)，這使得單目恢復尺度成爲可能；雖然我自己用OpenVINS的經驗這個尺度還蠻常被IMU noise影響的(打開方式不對？我沒研究過它初始化是不是用蘇黎世介紹的這套)。

## VSLAM框架

1. **傳感器訊號讀取**: 以相機爲主、IMU/光達/紅外線爲輔的讀取與預處理。這些不同的感測器之間最好可以有硬體同步；如果沒有的話你可能會需要更完備的算法，比方說[連續時域模型](https://youtu.be/_yuZmzJoWUc)之類的。

2. **前端視覺里程計**: 估算相鄰幀間相機的相對運動，以及局部地圖的樣子。因爲只有局部，不可避免地會有累積漂移的情形發生。

3. **後端最佳化**: 接受前端得到的相機姿態跟「其他約束」，對它們做一些事情(最佳化！)以得到全局一致的軌跡跟地圖。透過排除雜訊解決部分累積漂移。

4. **迴環檢測**: 判斷機器人是否達到先前的位置，有的話，把它加到後端的「其他約束」裏面一起大鍋炒。進一步解決累積漂移。

5. **建圖**: 根據定位的結果回推出一張地圖。本書以度量地圖(依幾何關係爲主)來介紹。這些地圖之後可以被其他導航算法利用，像A\*, D\*, RRT 之類的。

## SLAM問題的數學表述

運動方程:

$$
x_k = f(x_{k-1}, u_k, w_k)
$$

$k$爲時間點、$x_k$ 爲機器人位置、$u_k$是傳感器讀數、$w_k$是各種雜訊。

觀測方程:

$$
z_{k,j} = h(y_j, x_k, v_{k,j})
$$

一樣以$k$爲時間點、$y_j$爲第j個路標點、$v_{k,j}$是雜訊。

要將它們具體的表達出來，我們將它們**參數化**(parametrization)。比方說，我可以用極座標表示一個二維上的$x_k$，或者用笛卡爾座標表示$y_j$。這部分會在第三/四/五講詳細介紹。

於是V-SLAM就變成了一個狀態估計問題：*如何通過帶有雜訊的數據來估計內部的、隱藏的狀態變量？*

因爲SLAM是一個非線性非高斯系統，我們大致可以把它分成兩種方式：EKF或非線性最佳化。這部分會在第六講介紹。另外最近有基於等變濾波(equivariant filter)的解法，因爲它**真的是太快了**，所以我想額外詳讀它。不過要閱讀之前需要一些李群跟流形的預備知識，不知道到底能不能懂，唉。

## 實踐

教你怎麼在Linux用CMake做C++的Hello world，跳過。
另外介紹使用KDevelop作爲IDE開發。不過因爲版本跟語言不同，我實際用起來有點卡卡。

## 習題


